---
title: "Final Project"
output: pdf_document
---


**Your Name**: Anasa Alamgir
**Your G Number**: G01300460


```{r warning = FALSE, message = FALSE}
# Suppress dplyr summarise grouping warning messages
options(dplyr.summarise.inform = FALSE)

## Add R libraries here
library(tidyverse)
library(tidymodels)
library(discrim)
library(kknn)
library(dplyr)
library(klaR)
library(ggplot2)
library(vip)
library(magrittr)

# Load data
loans_df <- read_rds("loan_data.rds")

```
# Summary of Results [50 Points]

Write a summary of your overall findings and recommendations to the executives at the bank. Think of this section as your closing remarks of a presentation, where you summarize your key findings, model performance, and make recommendations to improve loan processes at the bank.

Your executive summary must be written in a business tone, with minimal grammatical errors, and should include the following sections:

1. An introduction where you explain the business problem and goals of your data analysis

    - What problem(s) is this company trying to solve? Why are they important to their future success?
  
    - What was the goal of your analysis? What questions were you trying to answer and why do they matter?


2. Highlights and key findings from your Exploratory Data Analysis section 
    - What were the interesting findings from your analysis and **why are they important for the business**?

    - This section is meant to **establish the need for your recommendations** in the following section


3. Your “best” classification model and an analysis of its performance 
    - In this section you should talk about the expected error of your model on future data
      - To estimate future performance, you can use your model performance results on the **test data**
    - You should discuss at least one performance metric, such as an F1 or ROC AUC for your model. However, you must explain the results in an **intuitive, non-technical manner**. Your audience in this case are executives at a bank with limited knowledge of machine learning.


4. Your recommendations to the company on how to reduce loan default rates 
  
    - Each recommendation must be supported by your data analysis results 

    - You must clearly explain why you are making each recommendation and which results from your data analysis support this recommendation

    - You must also describe the potential business impact of your recommendation:
      
      - Why is this a good recommendation? 
      
      - What benefits will the business achieve?

5. Conclusion

Wrap up the report with concluding remarks by summarizing the results and your recommendations in two or three paragraphs.

6. Appendix/Appendices

# Data Analysis [30 Points]

In this section, you must think of at least 6 relevant questions that explore the relationship between `loan_default` and the other variables in the `loan_df` data set. The goal of your analysis should be discovering which variables drive the differences between customers who do and do not default on their loans.

You must answer each question and provide supporting data summaries with either a summary data frame (using `dplyr`/`tidyr`) or a plot (using `ggplot`) or both.

In total, you must have a minimum of 3 plots (created with `ggplot`) and 3 summary data frames (created with `dplyr`) for the exploratory data analysis section. Among the plots you produce, you must have at least 3 different types (ex. box plot, bar chart, histogram, scatter plot, etc...)

See the example question below.


## Sample Question

**Are there differences in loan default rates by loan purpose?**

**Answer**: Yes, the data indicates that credit card and medical loans have significantly larger default rates than any other type of loan. In fact, both of these loan types have default rates at more than 50%. This is nearly two times the average default rate for all other loan types.


### Summary Table

```{r echo = TRUE, fig.height=5, fig.width=9}

loans_df %>%
  group_by(loan_purpose) %>% 
  summarise(n_customers = n(),
            customers_default = sum(loan_default == 'yes'),
            default_percent = 100 * mean(loan_default == 'yes'))
```


### Data Visulatization

```{r echo = TRUE, fig.height=5, fig.width=9}
default_rates <- loans_df %>%
                 group_by(loan_purpose) %>% 
                 summarise(n_customers = n(),
                 customers_default = sum(loan_default == 'yes'),
                 default_percent = 100 * mean(loan_default == 'yes'))


ggplot(data = default_rates, mapping = aes(x = loan_purpose, y = default_percent)) +
    geom_bar(stat = 'identity', fill = '#006EA1', color = 'white') +
    labs(title = 'Loan Default Rate by Purpose of Loan',
         x = 'Loan Purpose',
         y = 'Default Percentage') +
    theme_light()
```


```{r}
head(loans_df)
```



## Question 1


**Question**: Is there a relationship between loan default and loan amount?


**Answer**:


```{r, fig.asp=0.6, fig.width=8}
loans_df %>% ggplot(
  aes(x = loan_amount, fill = loan_default)
) +
  geom_histogram(bins=10, color="white") + 
  scale_fill_manual(values = c("#33CC66","#FF3399")) +
  theme_light() +
  facet_wrap(~ loan_default)+
  labs(
    title = "Loan default distribution by loan amount",
    x = "loan amount",
    y = "number of loans"
  )
```



## Question 2


**Question**: Is there a relationship between loan default and history of missed
payments in the past 2 years based on term?


**Answer**:


```{r}
loans_df %>%
  group_by(term, loan_default) %>%
  summarise(
    num_loans = n()
  )
```


## Question 3


**Question**: Is there a relationship between loan default and annual income?


**Answer**:


```{r}
loans_df %>% group_by(loan_default) %>%
  summarise(
    avg_income = mean(annual_income),
    min_income = min(annual_income),
    sd_income = sd(annual_income)
  )
```



## Question 4


**Question**: Is there a relationship between loan default and debt-to-income 
ratio?


**Answer**: 


```{r, fig.width=9, fig.asp=0.5}
loans_df %>% ggplot(
  aes(x = debt_to_income, fill = loan_default)
) + geom_histogram(bins=16, color = "white") + xlim(0,100)+ 
  theme_light() +
scale_fill_brewer(palette="Set1") +
  facet_wrap(~ loan_default)
```



## Question 5


**Question**: Is there a relationship between loan default and 
years of credit history?


**Answer**:


```{r}
loans_df %>% ggplot(
  aes(x = years_credit_history, fill = loan_default) 
) + theme_light() +
  geom_histogram(bins=15, color="white") + 
  facet_wrap(~loan_default)
```

## Question 6


**Question**: Is there a relationship between loan default rate and interest 
rates and loan purpose?


**Answer**:


```{r}
loans_df %>% group_by(loan_default, loan_purpose) %>%
  summarise(
    avg_interestrate = mean(interest_rate),
    max_interestrate = max(interest_rate)
  )
```


# Predictive Modeling [70 Points]


In this section of the project, you will fit **three classification algorithms** to predict the response variable,`loan_default`. You should use all of the other variables in the `loans_df` data as predictor variables for each model.

You must follow the machine learning steps below. 

The data splitting and feature engineering steps should only be done once so that your models are using the same data and feature engineering steps for training.

- Split the `loans_df` data into a training and test set (remember to set your seed)
- Specify a feature engineering pipeline with the `recipes` package
    - You can include steps such as skewness transformation, dummy variable encoding or any other steps you find appropriate
- Specify a `parsnip` model object
    - You may choose from the following classification algorithms:
      - Logistic Regression
      - LDA
      - QDA
      - KNN
      - Decision Tree
      - Random Forest
- Package your recipe and model into a workflow
- Fit your workflow to the training data
    - If your model has hyperparameters:
      - Split the training data into 5 folds for 5-fold cross validation using `vfold_cv` (remember to set your seed)
      - Perform hyperparamter tuning with a random grid search using the `grid_random()` function 
      - Hyperparameter tuning can take a significant amount of computing time. Be careful not to set the `size` argument of `grid_random()` too large. I recommend `size` = 10 or smaller.
      - Select the best model with `select_best()` and finalize your workflow
- Evaluate model performance on the test set by plotting an ROC curve using `autoplot()` and calculating the area under the ROC curve on your test data



## Model 1 Logistic Regression

```{r}
#split loans_df into training and test sets
set.seed(172)
loan_split <- initial_split(loans_df, prop = 0.75,
                            strata = loan_default)

loan_training <- loan_split %>% training()

loan_test <- loan_split %>% testing()
```

```{r}
#cross validation folds for hyperparameter tuning
set.seed(172) 
loan_folds <- vfold_cv(loan_training, v = 6)
```

```{r}
#feature engineering
loan_recipe <- recipe(loan_default ~ .,data = loan_training) %>%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())

loan_recipe %>%
  prep(training = loan_training) %>%
  bake(new_data = NULL)
```


```{r}
#specify model
loan_logistic <- logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')
```

```{r}
#create workflow
logistic_wf <- workflow() %>%
  add_model(loan_logistic) %>%
  add_recipe(loan_recipe)

#roc curve and auc
logistic_fit <- logistic_wf %>% last_fit(split = loan_split)

# collect predictions
logistic_predictions <- logistic_fit %>% collect_predictions()
```

```{r}
#roc curve and auc
roc_curve(logistic_predictions,
          truth = loan_default,
          estimate = .pred_yes) %>% autoplot()

roc_auc(logistic_predictions, truth = loan_default, .pred_yes)
```


```{r}
#confusion matrix
conf_mat(logistic_predictions,
         truth = loan_default,
         estimate = .pred_class)
```


```{r}
#model summary 
log_model <- glm(loan_default ~., data = loan_training, family = binomial())
tidy(log_model)
summary(log_model)
```

```{r}
vip(log_model)
```


## Model 2 LDA

```{r}
#specify model
loan_lda <- discrim_regularized(frac_common_cov = 1) %>%
  set_engine("klaR") %>%
  set_mode("classification")
```


```{r}
#workflow
lda_wf <- workflow() %>% add_model(loan_lda) %>% add_recipe(loan_recipe)
```

```{r}
#fit workflow
lda_fit <- lda_wf %>% last_fit(split=loan_split)
```

```{r}
#collect predictions
lda_predictions <- lda_fit %>% collect_predictions()
```

```{r}
#roc curve
roc_curve(lda_predictions, truth=loan_default, estimate= .pred_yes) %>%
  autoplot()
#auc
roc_auc(lda_predictions, truth=loan_default, .pred_yes)
```

```{r}
#confusion matrix
conf_mat(lda_predictions, truth = loan_default, estimate = .pred_class)
```



## Model 3 K-Nearest Neighbor 

```{r}
#specify model
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")
```


```{r}
#workflow
knn_wf <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(loan_recipe)
```



```{r}
#create grid for hyperparameter testing
k_grid <- tibble(neighbors = c(10,20,30,40,50,75,100,125,150))
```


```{r}
#tuning wf
set.seed(271)
knn_tuning <- knn_wf %>% tune_grid(resamples=loan_folds, grid=k_grid)

#select best model from tuning result
best_k <- knn_tuning %>% select_best(metric='roc_auc')

#add optimal model to wf
final_knn_wf <- knn_wf %>% finalize_workflow(best_k)

#fit model
knn_fit <- final_knn_wf %>% last_fit(split=loan_split)
```

```{r}
#get df of test prediction results
knn_predictions <- knn_fit %>% collect_predictions()
```

```{r}
#roc curve and roc auc and confusion matrix for predictions
roc_curve(knn_predictions, truth = loan_default, estimate= .pred_yes) %>%
  autoplot()

roc_auc(knn_predictions, truth = loan_default, estimate= .pred_yes)

conf_mat(knn_predictions, truth= loan_default, estimate = .pred_class)
```

--- End of the Project ---
